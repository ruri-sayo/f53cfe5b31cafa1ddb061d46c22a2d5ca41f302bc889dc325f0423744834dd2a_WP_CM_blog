---
layout: post
title: WSL学習用テキスト
categories: [技術]
---

# WSLマスターへの道：研究者のための実践ガイドブック

## はじめに：なぜ今、研究者がWSLを学ぶべきなのか

本書を手に取っていただき、ありがとうございます。この教材は、WindowsをメインのOSとして利用しながら、データ解析やプログラミングでLinux環境のパワーを活用したいと考えている学生や研究者のために執筆されました。特に、瑠璃さんのように機械工学や生命科学といった分野で、シミュレーションやバイオインフォマティクス解析を行う方にとって、強力な武器となる知識を提供することを目指します。

Windows Subsystem for Linux (WSL)は、まさに革命的な技術です。かつては、Windows上でLinux環境を整えるには、デュアルブートや仮想マシンといった手間のかかる方法しかありませんでした。しかしWSLの登場により、使い慣れたWindowsのデスクトップ環境から、シームレスにLinuxの強力なコマンドラインツールや開発環境へアクセスできるようになったのです。

本教材の最終的なゴールは、あなたの研究や開発における一連のワークフロー（データの取得、前処理、解析、可視化）を、WSL上でスムーズに完結できるスキルを習得することです。さあ、一緒にLinuxの世界へ踏み出しましょう。

# 第1部：基礎の確立（🟢 初級レベル）

> 目標：Linuxの世界に臆することなく、Windowsと同じ感覚で基本的なファイル操作やソフトウェア管理ができるようになる。

この部では、WSLの導入から始め、Windowsとの連携方法、そしてLinuxを操作する上で最も基本的なコマンドについて学びます。ここを乗り越えれば、Linux環境への心理的なハードルはぐっと下がるはずです。

## 第1章：最初の接点 - WSLの世界へようこそ

### 🎯 目標

> WSLがどのような技術であるかを理解する。<br>自身のPCにWSLとUbuntuをインストールし、起動できるようになる。

### 📖 解説

WSLとは何か？
WSL (Windows Subsystem for Linux) は、Windows上でネイティブにLinuxの実行ファイルを動かすための互換レイヤーです。特にWSL2は、内部で最適化された軽量な仮想マシン技術を使い、本物のLinuxカーネルを動作させています。これにより、従来の仮想マシン（VMwareやVirtualBoxなど）よりも高速に起動し、Windowsとの親和性も格段に高い、という利点があります。<br>
なぜ研究でUbuntuがよく使われるのか？
Linuxには様々な種類（ディストリビューション）がありますが、中でもUbuntuは世界中のデスクトップPCからサーバーまで、幅広く利用されています。その理由は以下の通りです。

- 豊富な情報： 利用者が多いため、Web上にチュートリアルやトラブルシューティングの情報が豊富にあります。
- 安定したパッケージ管理： aptという非常に優れたパッケージ管理システムにより、研究で用いる専門的なソフトウェアの導入やアップデートが容易です。
- コミュニティの活発さ： 世界中の開発者によって支えられており、継続的なアップデートとサポートが提供されています。<br>

生命科学の分野で開発される解析ツールの多くは、このLinux環境を前提として作られているため、WSLでUbuntuを動かすことは非常に合理的な選択です。

### 💻 コードを交えた解説

WSLのインストールは非常に簡単です。Windowsのターミナル（PowerShellまたはコマンドプロンプトを管理者として開いて）で、以下のコマンドを一行実行するだけです。
```
wsl --install
````

このコマンドは、WSLの有効化、最新のLinuxカーネルのダウンロード、そしてデフォルトのディストリビューションであるUbuntuのインストールまでを自動で行います。インストールが完了し、PCを再起動すると、Ubuntuの初期設定が始まります。ここで、Linux環境で使用するユーザー名とパスワードを設定します。このパスワードは後々何度も使いますので、忘れないようにしてください。

設定が完了すると、username@hostname:~$ のような表示（プロンプト）が現れます。これで、あなたはLinuxの世界に入りました。
正常に動作しているか確認するために、以下のコマンドを打ってみましょう。

` uname -a ` : 現在動作しているLinuxカーネルのバージョン情報を表示します。

`lsb_release -a`: 現在インストールされているディストリビューション（Ubuntu）のバージョン情報を表示します。

```
# 現在のカーネル情報を表示
uname -a
# -> Linux DESKTOP-XXXX 5.10.16.3-microsoft-standard-WSL2 ...

# ディストリビューション情報を表示
lsb_release -a
# -> No LSB modules are available.
# -> Distributor ID: Ubuntu
# -> Description:    Ubuntu 22.04.1 LTS
```

### ✏️ 練習問題

1. Windowsターミナルを管理者権限で開き、wsl --install を実行してUbuntuをインストールしましょう。
2. Ubuntuの初回起動時に、任意のユーザー名とパスワードを設定しましょう。
3. `uname -a` と `lsb_release -a` コマンドをそれぞれ実行し、表示される内容がどのような情報かを確認してみましょう。

## 第2章：2つの世界の架け橋 - WindowsとLinuxの連携

### 🎯 目標

> WindowsとWSLのファイルシステムの違いを理解する。<br>両方のシステム間でファイルを自由にやり取りできるようになる。

### 📖 解説

WSLの最大の強みは、Windowsとのシームレスな連携です。しかし、両者のファイルシステムの構造は根本的に異なります。この違いを理解することが、連携を使いこなす鍵となります。

- Windowsのファイルシステム: C: や D: といったドライブレターから始まり、各フォルダが \ (バックスラッシュ) で区切られます。（例: C:\Users\Ruri\Documents）

- Linuxのファイルシステム: すべてのファイルとディレクトリは、単一のルートディレクトリ / から始まります。Windowsのようなドライブレターは存在しません。（例: /home/ruri/documents）

WSLは、この違いを吸収するための仕組みを提供しています。<br>
WSLからWindowsへ: WindowsのCドライブは、WSL内の /mnt/c という特別な場所に自動的に「マウント」されます。これにより、WSLから cd /mnt/c/Users/... のようにしてWindows側のファイルにアクセスできます。<br>
WindowsからWSLへ: Windowsのエクスプローラーのアドレスバーに \\wsl$ と入力すると、ネットワークドライブのようにWSL内のファイルシステムにアクセスできます。<br>
また、パスの形式を相互に変換してくれる wslpath という便利なコマンドもあります。

### 💻 コードを交えた解説

実際にWindowsのデスクトップにあるファイルに、WSLからアクセスしてみましょう。
```
# WindowsのCドライブに移動
cd /mnt/c/

# Usersディレクトリに移動
cd Users/

# あなたのWindowsユーザー名のディレクトリに移動 (Tabキーで補完すると楽です)
cd YourWindowsUserName/

# Desktopに移動
cd Desktop/

# デスクトップにあるファイルやフォルダの一覧を表示
ls


逆も試してみましょう。WSLのホームディレクトリ（起動時の場所、通常は /home/<username>）にファイルを作成し、Windowsのエクスプローラーから見てみます。

# WSLのホームディレクトリに移動（どこにいてもこのコマンドで戻れます）
cd ~

# test_from_wsl.txt という名前の空のファイルを作成
touch test_from_wsl.txt
```

次に、Windowsのエクスプローラーを開き、アドレスバーに \\wsl$\Ubuntu\home\<あなたのUbuntuユーザー名> と入力してエンターキーを押してください。先ほど作成した test_from_wsl.txt が見えるはずです。

### ✏️ 練習問題
1.Windowsのデスクトップに、メモ帳などで hello_from_windows.txt というファイルを作成し、中に何か文字を書いて保存しましょう。<br>
2.WSLのターミナルから cat /mnt/c/Users/<あなたのWindowsユーザー名>/Desktop/hello_from_windows.txt を実行し、ファイルの内容が表示されることを確認しましょう。
3.WSLのホームディレクトリに touch hello_from_wsl.txt コマンドでファイルを作成し、Windowsのエクスプローラーで \\wsl$\Ubuntu を開いて、そのファイルが見えることを確認しましょう。

## 第3章：コマンドラインの作法 - 基本コマンド習得

### 🎯 目標

ターミナル操作の基本となる8つのコマンドをマスターし、CUI環境で自在にファイルやディレクトリを操作できるようになる。

### 📖 解説

GUI（グラフィカル・ユーザー・インターフェース）ではマウスを使ってファイル操作を行いますが、CUI（キャラクター・ユーザー・インターフェース）では、キーボードからコマンドを打ち込んでコンピュータに指示を出します。最初は戸惑うかもしれませんが、慣れるとGUIより遥かに高速で効率的な作業が可能です。ここでは、絶対に覚えておくべき必須コマンドを学びます。

### 💻 コードを交えた解説

pwd (Print Working Directory): 「今、自分はどこにいるのか？」を表示します。迷子になったらまずこれを打ちましょう。
```
pwd
# -> /home/ruri
```

ls (List): 「周りには何があるのか？」カレントディレクトリ（今いる場所）のファイルやディレクトリを一覧表示します。
- ls -l: 詳細表示（パーミッション、所有者、サイズ、更新日時など）
- ls -a: 隠しファイル（.で始まるファイル）もすべて表示
```
ls -la
# -> total 28
# -> drwxr-xr-x 1 ruri ruri 4096 Sep 12 01:23 .
# -> drwxr-xr-x 1 root root 4096 Sep 12 01:20 ..
# -> -rw------- 1 ruri ruri 1234 Sep 12 01:22 .bash_history
```

cd (Change Directory): ディレクトリを移動します。パスの指定方法には、ルートから記述する絶対パスと、現在地からの位置関係で記述する相対パスがあります。
```
# /usr/bin へ絶対パスで移動
cd /usr/bin

# ホームディレクトリへ戻る
cd ~

# my_research ディレクトリを作成
mkdir my_research

# my_research へ相対パスで移動
cd my_research
```

mkdir (Make Directory): 新しいディレクトリを作成します。
```
mkdir data scripts results
```

touch: 空のファイルを作成します。ファイルの最終更新日時を現在時刻に更新する目的でも使われます。
```
touch scripts/analysis.py
```

cp (Copy): ファイルやディレクトリをコピーします。
```
# ファイルを同じディレクトリに別名でコピー
cp scripts/analysis.py scripts/analysis_backup.py
```

mv (Move): ファイルやディレクトリを移動、または名前を変更します。
```
# ファイルを別のディレクトリに移動
mv analysis_backup.py results/

# ファイル名を変更
mv analysis.py main_analysis.py
```

rm (Remove): ファイルを削除します。rmコマンドで削除したファイルはゴミ箱には入らず、即座に消去されるので、細心の注意を払って使ってください。
```
rm -r: ディレクトリを中身ごと再帰的に削除します。

# ファイルを削除
rm results/analysis_backup.py

# ディレクトリを削除（中身が空である必要がある）
rmdir data
```

cat, less, head, tail: ファイルの中身を表示します。

cat: ファイル全体を一度に表示します。短いファイル向け。

less: ファイルをインタラクティブに閲覧します。巨大なログファイルなどを見るのに適しています。（qキーで終了）

head: ファイルの先頭部分（デフォルトは10行）を表示します。

tail: ファイルの末尾部分（デフォルトは10行）を表示します。

### ✏️ 練習問題
1. cd ~ でホームディレクトリに移動し、mkdir my_research でディレクトリを作成しましょう。
2.cd my_research で移動し、mkdir data scripts results で3つのサブディレクトリを作成しましょう。
3. touch scripts/analyze.sh で空のスクリプトファイルを作成しましょう。
4. cp scripts/analyze.sh scripts/backup_analyze.sh でファイルをコピーしましょう。
5. ls scripts を実行し、2つのファイルが存在することを確認しましょう。
6. rm scripts/backup_analyze.sh でバックアップファイルを削除し、ls scripts で消えたことを確認しましょう。

## 第4章：環境の拡張 - パッケージ管理と権限

### 🎯 目標

>パッケージマネージャー apt を使って、必要なソフトウェアを自分でインストールできるようになる。<br>
sudo の意味と、Linuxの基本的なファイル権限（パーミッション）について理解する。

### 📖 解説

Linux環境の大きな利点の一つが、aptに代表されるパッケージ管理システムの存在です。これは、ソフトウェアのインストール、アップデート、アンインストールを一元管理してくれるツールです。App StoreやGoogle Playストアのコマンドライン版を想像すると分かりやすいでしょう。

sudo とは？
sudo (Super User Do) は、コマンドをシステムの管理者権限（root権限）で実行するためのコマンドです。システムの根幹に関わる操作、例えばソフトウェアのインストールやシステム設定の変更などには、この sudo をコマンドの前に付ける必要があります。実行する際には、Ubuntuの初期設定で決めたパスワードの入力が求められます。

パーミッションとは？
Linuxは複数のユーザーが同時に利用することを前提に設計されたOSであり、ファイルやディレクトリごとに「誰が何をして良いか」という権限（パーミッション）が厳密に定められています。ls -lで表示される -rwxr-xr-x のような文字列がそれです。

最初の1文字: dならディレクトリ、-ならファイル。
-続く9文字: 3文字ずつのブロックで、左から「所有者」「グループ」「その他のユーザー」に対する権限を示します。

- r: 読み取り (read)
- w: 書き込み (write)
- x: 実行 (execute)

### 💻 コードを交えた解説

ソフトウェアをインストールする前には、まずパッケージリストを最新の状態にするのが作法です。
```
# パッケージリストを更新 (利用可能なパッケージのカタログを最新にする)
sudo apt update

# インストール済みのパッケージをアップグレード (実際に更新を適用する)
sudo apt upgrade
```

&& は、左のコマンドが成功したら、続けて右のコマンドを実行するという意味です。そのため、sudo apt update && sudo apt upgrade と一行で実行するのが一般的です。

では、実際に neofetch という、システムの情報をカッコよく表示してくれるツールをインストールしてみましょう。
```
sudo apt install neofetch
```

インストールが終わったら、neofetch と打って実行してみてください。

次に、パーミッションを変更してみましょう。chmod (Change Mode) コマンドを使います。
```
# test.sh という空のファイルを作成
touch test.sh

# 詳細表示でパーミッションを確認
ls -l test.sh
# -> -rw-r--r-- 1 ruri ruri 0 Sep 12 01:50 test.sh
# 所有者は読み書き(rw)可能だが、実行(x)はできない

# 所有者に実行権限(x)を付与する
chmod u+x test.sh

# 再度確認
ls -l test.sh
# -> -rwxr--r-- 1 ruri ruri 0 Sep 12 01:50 test.sh  (色も変わることが多い)
# 所有者の権限が rwx になった
```

数字を使って chmod 755 test.sh のように指定することもできます。これは r=4, w=2, x=1 の合計値で、所有者に7(rwx)、グループに5(r-x)、その他に5(r-x)の権限を与える、という意味です。自作のシェルスクリプトを実行可能にする際によく使います。

### ✏️ 練習問題

1. sudo apt update && sudo apt upgrade を実行し、システムを最新の状態に保ちましょう。
2. sudo apt install neofetch を実行し、システム情報を表示するツールをインストールしましょう。
3. インストール後、neofetch コマンドを実行して結果を確認しましょう。
4. touch test.sh でファイルを作成し、ls -l test.sh で初期の権限を確認しましょう。
5. chmod 755 test.sh を実行して実行権限を付与し、再度 ls -l test.sh を実行して権限表記が変わったことを確認しましょう。

## 第5章：開発効率の向上 - VSCodeとの連携

### 🎯 目標

> VSCodeとWSLを連携させ、Windowsの快適なGUIエディタでLinux上のファイルを直接編集・実行できる環境を構築する。

### 📖 解説

コマンドライン操作は強力ですが、本格的なコーディングには高機能なエディタが欠かせません。そこで登場するのが、Microsoftが開発したVisual Studio Code (VSCode) と、その拡張機能「Remote - WSL」です。

この拡張機能は、VSCodeのGUI部分をWindowsで動かしつつ、ファイル処理やプログラムの実行、デバッグといった機能（いわばエディタの頭脳）をWSL側で動かすという仕組みになっています。これにより、あたかもLinux上でVSCodeを直接動かしているかのような、完全に統合された開発体験が得られます。

💻 コードを交えた解説

まず、Windows側にVSCodeをインストールし、拡張機能マーケットプレースから「Remote - WSL」をインストールしてください。

準備ができたら、WSLのターミナルから操作します。
```
# 開発用のディレクトリを作成して移動
mkdir ~/vscode_project
cd ~/vscode_project

# カレントディレクトリをVSCodeで開く
code .
```

code . というコマンドを実行すると、Windows側でVSCodeが起動し、自動的にWSL環境に接続します。初回はWSL側へのコンポーネントのインストールが走るため少し時間がかかります。

VSCodeが起動したら、左下の緑色の部分に「WSL: Ubuntu」のように表示されていることを確認してください。これが接続中の証です。

あとは、VSCodeのファイルエクスプローラーで新しいファイルを作成し、コードを書いてみましょう。例えば hello.py を作成し、以下のように記述します。
```
# hello.py
import platform

print("Hello from VSCode on WSL!")
print(f"This script is running on: {platform.system()}")
```

ファイルを保存したら、VSCodeの統合ターミナル（Ctrl + @キーで開閉）を開きます。このターミナルは、WSLのbashに直接接続されています。ここでプログラムを実行します。
```
python3 hello.py
# -> Hello from VSCode on WSL!
# -> This script is running on: Linux
```

見事に、Windows上のVSCodeから、WSL内のPythonインタプリタを使ってスクリプトを実行できました。

### ✏️ 練習問題
1. WindowsにVSCodeをインストールし、拡張機能マーケットプレースから「Remote - WSL」をインストールしましょう。
2. WSLのターミナルで mkdir ~/vscode_test && cd ~/vscode_test を実行し、テスト用のディレクトリを作成して移動しましょう。
3. code . コマンドを実行し、VSCodeが起動してWSLに接続されることを確認しましょう。
4. VSCode内で新しいファイル app.js を作成し、console.log("Hello from Node.js on WSL!"); と記述して保存しましょう。
5. VSCodeの統合ターミナルで node app.js を実行し、結果が表示されることを確認しましょう。（node が未インストールの場合は sudo apt install nodejs でインストールしてください）

# 第2部：実践力の養成（🟡 中級レベル）
## 作用注：正直、ここから先は必要性が薄いと思ってる。
> 目標：自身の研究・開発分野で必要な環境をWSL上に構築し、日常的なタスクを自動化・効率化できるようになる。

この部では、より実践的な内容に踏み込みます。PythonやRといった解析言語の環境構築、Gitによるバージョン管理、そして定型作業を自動化するシェルスクリプトの作成方法を学び、研究活動におけるWSLの価値を本格的に引き出していきます。

## 第6章：再現可能な解析環境 - Python, R, Git

### 🎯 目標
> conda を用いて、プロジェクトごとに独立したPythonの仮想環境を構築・管理できるようになる。<br>
WSL上にRの解析環境を構築できるようになる。<br>
git を用いた基本的なバージョン管理（クローン、コミット、プッシュ）ができるようになる。

### 📖 解説

研究や開発を進める上で、「環境の再現性」は極めて重要です。半年前の自分の解析が、今実行するとエラーで動かない、共同研究者にコードを渡したら、ライブラリのバージョン違いで動かない、といった事態は頻繁に発生します。これを防ぐのが、仮想環境とバージョン管理の考え方です。

**なぜ仮想環境が必要か？**
プロジェクトAではライブラリXのバージョン1.0が、プロジェクトBではバージョン2.0が必要になる、という状況はよくあります。PC全体で一つの環境を共有していると、このような競合に対応できません。仮想環境は、プロジェクトごとに独立した部屋（環境）を用意するようなものです。各部屋では、他の部屋に影響を与えることなく、好きなバージョンのライブラリを自由にインストールできます。生命科学や機械学習の分野では、conda というツールがこの目的で広く使われています。

**バージョン管理システムGitとは？**
Gitは、ファイルの変更履歴を記録・追跡するためのシステムです。レポートを「report_v1.docx」「report_v2.docx」「report_final.docx」「report_final_revised.docx」のように手動で保存していく作業を、より高度かつ体系的に行ってくれるツールだと考えてください。Gitを使うことで、以下のメリットが得られます。
- 変更履歴の記録： 「いつ、誰が、どこを、なぜ変更したのか」を記録できます。
- 過去のバージョンへの復元： コードを大胆に変更して失敗しても、ボタン一つで正常に動いていた過去の状態に戻せます。
- 共同作業の効率化： 複数人での開発において、誰がどの部分を変更したかを容易に把握し、変更点を統合（マージ）できます。

### 💻 コードを交えた解説

1. conda によるPython環境構築
まず、condaのインストーラーであるMinicondaをWSLに導入します。

```
# インストーラーをダウンロード
wget [https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh)

# インストーラーを実行
bash Miniconda3-latest-Linux-x86_64.sh

# ターミナルを再起動して設定を反映
# (または source ~/.bashrc を実行)
```

インストール中に何度か質問されますが、基本的にはEnterキーを押して進み、ライセンス同意(yes)とconda initの実行(yes)を選択してください。

インストール後、bio-envという名前で、Python 3.9と基本的なデータサイエンスライブラリを含む新しい仮想環境を作成してみましょう。

``` 
# 'bio-env'という名前で仮想環境を作成
conda create --name bio-env python=3.9 pandas jupyterlab

# 作成した環境を有効化（アクティベート）する
conda activate bio-env

# プロンプトの先頭に (bio-env) と表示され、環境が変わったことがわかる
# (bio-env) ruri@hostname:~$

# この環境から抜ける（非アクティブ化）
conda deactivate
```

2. Rの環境構築
次に、統計解析言語Rをインストールします。これはaptを使ってシステムに直接インストールするのが一般的です。

```
# Rの基本パッケージをインストール
sudo apt install r-base

# Rを起動
R
```

Rコマンドで対話環境が起動します。ここでinstall.packages("tidyverse")のように入力すれば、データサイエンスで人気のtidyverseパッケージ群をインストールできます。

3. GitとGitHubの基本サイクル
まず、GitHub上で新しいリポジトリ（例: wsl-research-project）を作成してください。

次に、WSLのターミナルで以下の操作を行います。

```
# Gitの初期設定（初回のみ）
git config --global user.name "Your Name"
git config --global user.email "you@example.com"

# GitHubからリポジトリをローカルにコピー（クローン）
# URLはご自身のものに置き換えてください
git clone [https://github.com/YourName/wsl-research-project.git](https://github.com/YourName/wsl-research-project.git)

# 作成されたディレクトリに移動
cd wsl-research-project/

# 新しいファイルを作成
echo "# My Research Project" > README.md

# 変更をステージングエリアに追加
git add README.md

# 変更内容を記録（コミット）
git commit -m "Add initial README file"

# GitHubにリモートリポジトリに変更を反映（プッシュ）
git push
```

この add -> commit -> push の流れが、Gitを使った開発の最も基本的なサイクルです。

### ✏️ 練習問題
1. MinicondaをWSLにインストールし、ターミナルを再起動して conda コマンドが使えることを確認しましょう。
2. conda create --name ml-env python=3.10 scikit-learn matplotlib というコマンドで、機械学習用の新しい仮想環境を作成し、アクティベートしてみましょう。
3. sudo apt install r-base でRをインストールした後、Rの対話環境を起動し、install.packages("ggplot2") を実行して作図パッケージをインストールしてみましょう。（q()でRを終了できます）
4. ご自身のGitHubアカウントに my-first-repo という名前で新しいリポジトリを作成し、WSLに git clone しましょう。
5. cloneしたディレクトリ内で touch first_file.txt というファイルを作成し、git add, git commit -m "Add first file", git push の一連の流れを実行して、GitHub上でファイルが作成されていることを確認しましょう。

## 第7章：定型作業の自動化 - シェルスクリプト入門

### 🎯 目標

> 複数のコマンドをまとめた「シェルスクリプト」を作成し、実行できるようになる。<br>
シェルスクリプト内で変数、forループ、if文を使い、より高度な自動化処理を記述できるようになる。

### 📖 解説

研究活動では、「大量のデータファイルに対して、同じ一連の処理を繰り返し行う」という場面が頻繁に発生します。例えば、100個のFASTQファイルに対して、品質チェック、トリミング、マッピングという3つのコマンドを順番に実行する場合、手作業で300回コマンドを打つのは現実的ではありません。

このような定型作業を自動化するのがシェルスクリプトです。シェルスクリプトとは、ターミナルで実行する一連のコマンドを、一つのテキストファイルにまとめたものです。このファイルに実行権限を与えて実行するだけで、記述されたコマンドが上から順番に自動で実行されます。

さらに、変数やループ、条件分岐といったプログラミングの要素を組み合わせることで、単なるコマンドの羅列に留まらない、柔軟で強力な自動化ツールを作成することができます。

### 💻 コードを交えた解説

1. はじめてのシェルスクリプト
まず、非常に簡単なスクリプトを作成してみましょう。hello.sh という名前のファイルを作成し、VSCodeなどのエディタで以下の内容を記述します。

```
#!/bin/bash

# このスクリプトは "Hello, World!" と表示します
echo "Hello, Shell Script!"
echo "Today is $(date)"
```

- #!/bin/bash: "おまじない"のようなもので、このスクリプトをbashというシェルで実行することを宣言しています。
- #: #で始まる行はコメントとして扱われ、実行時には無視されます。
- echo: 後に続く文字列や変数の内容を画面に表示するコマンドです。
- $(date): dateコマンドを実行し、その実行結果（現在日時）をこの場所に展開します。

スクリプトを実行するには、まず実行権限を与える必要があります。

```
# 実行権限を付与
chmod 755 hello.sh

# スクリプトを実行
./hello.sh

# -> Hello, Shell Script!
# -> Today is Fri Sep 12 01:49:17 JST 2025

```

./ は「カレントディレクトリにある」という意味です。これがないと、システムは別の場所（$PATH環境変数で指定されたディレクトリ）にhello.shコマンドを探しに行ってしまい、見つけられません。

2. forループを使った繰り返し処理
forループを使うと、複数のファイルに対して同じ処理を簡単に行えます。

```
#!/bin/bash

# dataディレクトリにある、.fastqで終わる全てのファイルに対してループ処理
for file in data/*.fastq
do
  echo "------------------------------"
  echo "Processing file: $file"
  # ここに実際の解析コマンドを書く
  # 例: fastqc $file
  echo "Done."
done

echo "All jobs finished!"
```

$file の部分が、ループの各回で data/sample1.fastq, data/sample2.fastq, ... のように置き換わっていきます。`$fileのように、$`を先頭につけることで変数の値を参照できます。

3. if文を使った条件分岐
if文を使うと、「もし〜〜ならば、この処理を実行する」という条件に応じた処理の分岐ができます。例えば、「処理結果を保存するディレクトリがなければ作成する」といった処理に役立ちます。

```
#!/bin/bash

OUTPUT_DIR="results"

# resultsディレクトリが存在しないかチェック
if [ ! -d "$OUTPUT_DIR" ]; then
  echo "Output directory '$OUTPUT_DIR' not found. Creating it..."
  mkdir "$OUTPUT_DIR"
else
  echo "Output directory '$OUTPUT_DIR' already exists."
fi
```

- OUTPUT_DIR="results": OUTPUT_DIRという名前の変数にresultsという文字列を代入しています。
- ` if [ ! -d "＄OUTPUT_DIR" ] ` : -dはディレクトリかどうかをチェックする演算子、!は否定を意味します。つまり「もし$OUTPUT_DIRがディレクトリでなければ」という条件です。変数を"で囲むのは、ファイル名にスペースが含まれる場合などに備えた良い習慣です。

### ✏️ 練習問題
1. data というディレクトリを作成し、その中に touch sample1.fastq sample2.fastq sample3.fastq コマンドで3つの空のファイルを作成しましょう。
2. for file in data/*.fastq; do echo "File found: $file"; done というforループを、スクリプトファイルにはせず、ターミナルに直接打ち込んで実行してみましょう。
3. 上記のプロットに記載した process.sh のようなスクリプトを作成し、実行権限を与えて ./process.sh で実行してみましょう。
4. （応用）process.sh を改良し、「results ディレクトリが存在しなければ作成する」というif文を追加してみましょう。そして、ループ処理の中で、各ファイルの処理結果（の代わり）として touch results/$(basename $file).processed というコマンドを実行し、results ディレクトリに処理済みファイルが作成されるようにしてみましょう。（basenameはパスからファイル名だけを取り出すコマンドです）

## 第8章：解析パイプラインの実行 - バイオインフォマティクスツール

### 🎯 目標

> conda を利用して、専門的なバイオインフォマティクスツールをインストールできるようになる。<br>
パイプ(｜)とリダイレクト(＞)を使いこなし、コマンドを繋げて効率的な処理を行えるようになる。<br>
WSL上でJupyterLabやRStudio Serverを起動し、Windowsのブラウザからアクセスできるようになる。

### 📖 解説

ここまでの章で学んだ知識を組み合わせることで、いよいよ実際の研究に近い、データ解析のパイプライン（一連の処理の流れ）をWSL上で実行できます。生命科学の分野では、膨大な数のオープンソースツールが開発されており、その多くはLinux環境での利用が前提となっています。WSLは、これらのツールをWindows上で利用するための、まさに最適なプラットフォームです。

**パイプとリダイレクト**
- Linuxコマンドラインの哲学の一つに、「一つのことをうまくやる、小さなプログラムを組み合わせる」というものがあります。この「組み合わせ」を実現するのが、**パイプ(｜)とリダイレクト(＞)**です。
- パイプ (|): あるコマンドの標準出力（画面に表示される結果）を、別のコマンドの標準入力（キーボードからの入力）に直接繋ぎます。これにより、中間ファイルを作成することなく、複数の処理を連続して実行できます。
- リダイレクト (>): コマンドの標準出力を、画面に表示する代わりにファイルに書き出します。処理結果を保存する際に頻繁に利用します。>はファイルを上書きし、>>はファイルに追記します。

**Webベースの解析環境**
JupyterLabやRStudioは、データ分析において非常に人気のある対話的な開発環境です。WSL上でこれらのサーバーを起動し、Windows側のWebブラウザから localhost (自分自身のPCを指す特別なアドレス) を通じてアクセスすることで、Windowsの快適なGUIとWSLの強力な計算環境を両立させることができます。この時、WSL内で動いているサーバーの特定の「ポート番号」と、Windows側が通信できるように、WSLが自動的にポートフォワーディングを行ってくれています。

### 💻 コードを交えた解説

1. バイオインフォマティクスツールのインストール
第6章で作成したcondaのbio-env環境に必要なツールを追加します。biocondaという、生命科学分野のツールに特化したチャンネルを指定するのが一般的です。

```
# conda環境を有効化
conda activate bio-env

# biocondaチャンネルを追加して、fastqcとsamtoolsをインストール
conda install -c bioconda fastqc samtools
```

2. パイプとリダイレクトの実践
これらの機能を、すでにお馴染みのコマンドで試してみましょう。

```
# ls -l の出力（詳細なファイルリスト）を...
# head -n 5 の入力に繋ぎ、先頭5行だけを表示する
ls -l /etc/ | head -n 5

# ls -l の出力を、画面ではなく file_list.txt というファイルに書き出す
ls -l > file_list.txt

# catでファイルの中身を確認
cat file_list.txt

# dateコマンドの出力を、file_list.txt に追記する
date >> file_list.txt

# 再度、中身を確認
cat file_list.txt
```

このように、小さなコマンドを繋ぎ合わせることで、複雑な処理を実現できるのがLinuxの強力な点です。

3. JupyterLabの起動とブラウザからのアクセス
bio-env環境が有効化されていることを確認して、JupyterLabを起動します。

```
# (bio-env) ruri@hostname:~$

# JupyterLabを起動
jupyter lab
```

起動すると、ターミナル上に以下のようなメッセージが表示されます。

```
To access the server, open this file in a browser:
    file:///home/ruri/.local/share/jupyter/runtime/jpserver-12345-open.html
Or copy and paste one of these URLs:
    http://localhost:8888/lab?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 or [http://127.0.0.1:8888/lab?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx](http://127.0.0.1:8888/lab?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)

```

この http://localhost:8888/lab?token=... から始まるURLを Ctrlキーを押しながらクリックするか、コピーしてWindowsのWebブラウザ（ChromeやEdgeなど）のアドレスバーに貼り付けてください。すると、ブラウザ上でJupyterLabのインターフェースが開きます。

これで、WSLの計算リソースを使いながら、Windowsの使い慣れたブラウザでPythonのコーディングとデータ分析ができます。

### ✏️ 練習問題
1. conda activate bio-env で環境を有効化した後、conda install -c bioconda bwa コマンドで、代表的なマッピングツールであるBWAをインストールしてみましょう。
2. ls /bin ｜ sort ｜ head -n 10 というコマンドを実行してみましょう。/bin ディレクトリのファイルリストを、アルファベット順にソート(sort)し、その結果の先頭10行を表示する、というパイプを使った処理です。<br>
※作用注：｜が全角になっているので注意
3. env > environment_variables.txt というコマンドを実行してみましょう。現在の環境変数の一覧をファイルに保存するリダイレクト処理です。catで中身を確認してみてください。
4. （WSL上で）conda activate bio-env を実行した後、jupyter lab を起動し、表示されたURLをWindowsのブラウザに貼り付けて、JupyterLabにアクセスできることを確認してみましょう。

## 第9章：究極の再現性 - Dockerコンテナ入門

### 🎯 目標

> Dockerコンテナがどのような技術であり、なぜ研究の再現性確保に重要なのかを理解する。<br>
WSL2と連携したDocker Desktopを導入し、基本的なDockerコマンドを実行できるようになる。

### 📖 解説

**Dockerとは何か？**<br>
 Dockerは、「コンテナ」と呼ばれる、OSレベルの軽量な仮想環境を作成・配布・実行するためのプラットフォームです。従来の仮想マシンがOS全体をエミュレートするのに対し、コンテナはホストOSのカーネルを共有し、アプリケーションとその依存ライブラリ群だけを隔離した環境で動かします。これにより、仮想マシンよりも遥かに高速・軽量に動作します。

スキーに例えるなら、仮想マシンが「自分専用のスキー場を丸ごと一つ建設する」ようなものだとすれば、コンテナは「必要なコースとリフトだけがパッケージ化された、移動式のミニゲレンデ」のようなものです。どこにでも簡単に持ち運べ、すぐに同じ環境で滑り出すことができます。

**なぜ解析環境のコンテナ化が重要なのか？**
<br>学術研究において、「論文に書かれた通りの手順を追えば、誰もが同じ結果を再現できる」という再現性は、その研究の信頼性を担保する上で生命線です。しかし、解析に使用したツールのバージョン、ライブラリの依存関係、OSの違いなど、環境の僅かな差異が結果を大きく変えてしまうことがあります。

Dockerは、解析に用いた環境（OS、ツール、ライブラリ、設定ファイルなど）を「Dockerイメージ」という一つの固まりとしてパッケージ化し、共有することを可能にします。これにより、共同研究者や論文の読者は、あなたの解析環境を数分で、かつ寸分違わぬ形で自身のPC上に再現できるのです。

### 💻 コードを交えた解説

1. Docker DesktopのインストールとWSL2連携 まず、WindowsにDocker Desktopをインストールします。公式サイトからインストーラーをダウンロードしてください。

Docker Desktop 公式サイト

インストール中に「Use WSL 2 instead of Hyper-V」のオプションが表示されたら、必ずチェックを入れてください。インストール後、Docker Desktopを起動し、設定画面（歯車アイコン）を開きます。 Settings > Resources > WSL Integration の項目で、Enable integration with my default WSL distro がオンになっていること、そしてUbuntuが有効になっていることを確認してください。

2. 基本的なDockerコマンド 設定が完了したら、WSLのターミナルを開きます。Docker Desktopが起動していれば、WSL内からdockerコマンドが利用できます。

```
# Dockerが正常に動作しているか確認
docker --version
# -> Docker version 24.0.5, build ...

# "hello-world"イメージを実行する（最も基本的な動作確認）
# イメージがローカルになければ、自動的にダウンロード(pull)してから実行(run)する
docker run hello-world
```

Hello from Docker! というメッセージが表示されれば成功です。

次に、バイオインフォマティクスでよく使われるsamtoolsの公式イメージを使ってみましょう。

```
# Docker Hubからsamtoolsのイメージをダウンロード(pull)する
docker pull bioconda/samtools

# ダウンロード済みのローカルイメージ一覧を表示
docker images
# -> REPOSITORY          TAG       IMAGE ID       CREATED        SIZE
# -> bioconda/samtools   latest    xxxxxxxxxxxx   2 weeks ago    500MB
# -> hello-world         latest    xxxxxxxxxxxx   3 months ago   9.14kB

# samtoolsのコンテナを起動し、中に入る(-it)
# コンテナが終了したら自動で削除する(--rm)
# コンテナ内でbashを起動する
docker run --rm -it bioconda/samtools bash

# コンテナ内のプロンプトに変わる
# root@xxxxxxxxxxxx:/data#

# コンテナ内でsamtoolsコマンドを実行してみる
samtools --version
# -> samtools 1.15.1
# -> Using htslib 1.15.1

# exitと打つとコンテナから抜け、コンテナは削除される
exit
```

このように、conda installなどでホスト環境を汚すことなく、必要なツールが完璧にセットアップされた環境を、コマンド一つで呼び出して利用できるのがDockerの強力な点です。

### ✏️ 練習問題
1. Docker Desktop for Windowsをインストールし、設定画面でWSL2との連携が有効になっていることを確認しましょう。
2. WSLのターミナルで docker run hello-world を実行し、Dockerが正常に動作することを確かめましょう。
3. docker pull ubuntu:22.04 コマンドで、最新のUbuntuの公式イメージを取得してみましょう。
4. docker run --rm -it ubuntu:22.04 bash を実行して、"まっさらな"Ubuntuのコンテナの中に入ってみましょう。lsやcat /etc/os-releaseなどのコマンドを実行し、ホストのWSL環境とは隔離されていることを体感してください。（exitでコンテナを終了できます）

## 第10章：長時間計算との付き合い方 - ジョブ管理の初歩

### 🎯 目標
> 時間のかかる処理をバックグラウンドで実行できるようになる。<br>
ターミナルの接続が切れても処理を継続させるツールtmuxの基本操作を習得する。<br>
htopコマンドを使い、システムのCPUやメモリの使用状況を監視できるようになる。

### 📖 解説

シーケンスデータのマッピングや機械学習モデルのトレーニングなど、研究における計算処理には数時間、時には数日かかるものも珍しくありません。このような長時間ジョブを実行している間、ターミナルを開きっぱなしにしておくのは非効率ですし、ネットワークの切断やPCのスリープなどで処理が中断してしまうリスクもあります。

ここでは、大規模な計算機サーバー（スパコン）などで使われるジョブ管理の初歩的なテクニックを学び、WSL上で長時間計算を賢く実行する方法を身につけます。

#### バックグラウンド実行 (&, nohup)

&: コマンドの末尾に&を付けると、そのコマンドはバックグラウンドで実行され、すぐにターミナルの操作が返ってきます。ただし、この状態ではターミナルを閉じると処理も一緒に終了してしまいます。

nohup: nohup (No Hang Up) コマンドと一緒に実行すると、ターミナルを閉じても処理が終了しなくなります。バックグラウンドジョブの実行に非常に便利です。

セッション管理 (tmux)
tmuxは、一つのターミナルウィンドウ内で複数の仮想的なターミナル（セッション）を作成し、それらを自由に切り替えたり、切り離したり（デタッチ）、再接続したり（アタッチ）できるツールです。
tmuxのセッション内で長時間かかるコマンドを実行しておけば、ターミナルを閉じてもセッションはバックグラウンドで生き続けます。後で再びターミナルを開き、そのセッションにアタッチすれば、何事もなかったかのように処理の続きから操作を再開できます。

リソース監視 (htop)
htopは、CPUやメモリの使用率、実行中のプロセスなどを対話的に表示してくれるツールです。topコマンドの進化版で、よりカラフルで直感的にシステムの負荷状況を把握できます。自分の実行したジョブがどれくらいの計算資源を消費しているかを確認するのに役立ちます。

### 💻 コードを交えた解説

まず、tmuxとhtopをインストールします。

```
sudo apt update
sudo apt install tmux htop
```

1. バックグラウンド実行
sleepコマンド（指定した秒数だけ待機する）を使って、長時間ジョブを疑似的に再現します。

```
# 300秒(5分)待つ処理をバックグラウンドで実行
sleep 300 &

# [1] 12345 のようにジョブIDとプロセスIDが表示され、すぐにプロンプトが返ってくる
# -> [1] 12345

# 現在実行中のバックグラウンドジョブ一覧を表示
jobs
# -> [1]+  Running                 sleep 300 &
```

2. tmuxの基本操作
tmuxの操作は、Ctrl+b（これをプレフィックスキーと呼びます）を押した後に、別のキーを入力することで行います。

```
# 新しいtmuxセッションを開始
tmux

# (tmuxセッションの中に入る。画面下部に緑色のステータスバーが表示される)

# ここで長時間かかるコマンドを実行する
# 例: sleep 1000

# セッションから離れる（デタッチする）
# Ctrl+b を押してから、d キーを押す
```

デタッチすると元のターミナルに戻りますが、sleep 1000の処理はtmuxセッションの中で生き続けています。ターミナルを一度閉じ、再度開いてから以下のコマンドを実行します。
```
# 現在存在するtmuxセッションの一覧を表示
tmux ls
# -> 0: 1 windows (created Fri Sep 12 02:20:00 2025)

# 最後のセッションに再接続（アタッチ）する
tmux attach

# (先ほどのセッションに戻り、sleep 1000がまだ実行中であることが確認できる)
```

3. htopによるリソース監視
単純にコマンドを実行するだけです。

```
htop
```

CPUのコアごとの使用率、メモリ使用量、実行中プロセスのリストなどが表示されます。矢印キーでリストをスクロールしたり、F9キーでプロセスを終了させたりできます。自分の実行した解析ジョブがどの程度CPUを使っているかなどを確認するのに便利です。（qキーで終了します）

### ✏️ 練習問題
1. sleep 300 & を実行し、プロセスをバックグラウンドで実行しましょう。その後、jobs コマンドでジョブが"Running"状態であることを確認しましょう。
2. tmux を起動し、htop を実行してみましょう。その後、Ctrl+b -> d でデタッチし、一度ターミナルを閉じてから再度開き、tmux attach でhtopが動き続けているセッションに戻れることを体験しましょう。（qでhtopを終了し、exitでtmuxセッションを終了できます）
3. （応用）nohup sleep 600 > sleep.log & というコマンドを実行してみましょう。これは、ターミナルを閉じても処理が継続し(nohup)、かつ画面出力をsleep.logというファイルに書き出す(>)コマンドです。実行後にターミナルを閉じ、数分待ってから再度開き、cat sleep.logで中身を確認してみてください。

