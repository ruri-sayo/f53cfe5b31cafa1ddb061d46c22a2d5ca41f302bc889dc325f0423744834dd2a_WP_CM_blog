---
layout: post
title: nano-banana proが発表されたね
date:   2025-11-22 2:10:00 +0900
categories: [技術]
description: googleが発表した、新しい画像生成AIのnano-banana proについてまとめました。
---

# なにこれ？

「文字が書けて、ネット検索もできて、修正指示も完璧に通じる、超ハイスペックな専属デザイナー」
この表現が適切だと思う。
要は、これまでの画像生成AIに比べて、性能とユーザ体験が圧倒的に進化しちゃった。

# 何がすごい？

## 1. 画像の中の「文字」が完璧に書ける！

これまでの画像生成AIは、絵はうまくても、看板やポスターの中の文字が「謎の宇宙語」みたいにぐちゃぐちゃになるのが弱点でした。
でも、Nano Banana Proは「日本語の漢字」や「英語のスペル」を、間違えずにキレイに画像の中に書き込めます。

- **例:** 「文化祭のポスターを作って。真ん中に『第50回 輝け青春』と大きく書いて」と頼めば、その通りの文字が入ったポスター画像が一発で作れます。<br>
※実際に実行した例：プロンプト＝文化祭のポスターを作って。真ん中に『第50回 輝け青春』と大きく書いて
<img src="{{ '/assets/img/genAI_image/bunnkasai.webp' | relative_url }}" alt="プロンプト = 文化祭のポスターを作って。真ん中に『第50回 輝け青春』と大きく書いて" >


## 2. 「最新情報」を取り込んだ画像が作れる！

Google検索とつながっているので、**今のリアルタイムな情報を画像に反映**できます。

- **例:** 「今日の東京の天気を反映した、スマホの壁紙を作って」と頼むと、本当に今日の天気を調べて、晴れなら晴れの、雨なら雨の雰囲気の画像を生成してくれます。スポーツの試合結果をグラフにした画像なども作れます。

※実際に作成した例：プロンプト＝今日の東京の天気を反映した、スマホの壁紙を作って<br>
※今日の東京は、晴れ、最高気温16度予想

<img src="{{ '/assets/img/genAI_image/sumaho.webp' | relative_url }}" alt="プロンプト = 今日の東京の天気を反映した、スマホの壁紙を作って" >

## 3. 「あとから修正」が言葉だけで通じる！

画像を作った後に、「あ、惜しい！」と思うことってありますよね。それをまるで友達に頼むように直せます。

- **例:**
    - 「このキャラクター、服を赤から青に変えて」
    - 「カメラのアングルを、もっと上から撮った感じにして」
    - 「背景だけ土砂降りにして」
    これらを、**描き直しではなく「修正」として自然にやってくれます。**

※編集前：実際に作成した画像：プロンプト＝2足歩行のコアラを描いてください。コアラは、赤色のアロハシャツを着ています。
<img src="{{ '/assets/img/genAI_image/koara_aka.avif' | relative_url }}" alt="プロンプト = 2足歩行のコアラを描いてください。コアラは、赤色のアロハシャツを着ています。" >


※編集後：プロンプト＝このキャラクター、服を赤から青に変えて

<img src="{{ '/assets/img/genAI_image/koara_ao.avif' | relative_url }}" alt="プロンプト = このキャラクター、服を赤から青に変えて" >

※編集後：プロンプト＝カメラのアングルを、もっと上から撮った感じにして

<img src="{{ '/assets/img/genAI_image/koara_ue.avif' | relative_url }}" alt="プロンプト = カメラのアングルを、もっと上から撮った感じにして" >

※編集後：プロンプト＝背景だけ土砂降りにして

<img src="{{ '/assets/img/genAI_image/koara_ame.avif' | relative_url }}" alt="プロンプト = プロンプト＝背景だけ土砂降りにして" >

# 個人的な体感

これまでは、体感、画像を0→1にするのは、chatGPTのほうが取り回しが良かったように感じてた。<br>
nano-banana（旧モデル）も編集能力はすごかったが、文字の解釈モデルがflash（精度より速度を重視したモデル）だったこともあり、「なんか、違うよなぁ」が多発していた。<br>
また、日本語に関しても、文字生成が苦手な印象を受けていた。

それが、今回のアップデートで完全に覆された印象である。<br>
なんというか、0→1の生成も8→10の編集もどちらも対応する「個人用デザイナー」がリリースされたような感覚である。

学生はGemini proが使い放題なので、是非使い倒すべきだと思う。今回のGemini3.0アップデートでGemini系列は圧倒的に強く、使いやすくなった。

少し、本題と離れるが、AIに関する残酷な話をしておきたい。<br>
私は中の上ぐらいには文字入力が早いと思う。それでも、実際は、4.5～5キー/secondが限界である。<br>
仮に、1万文字を作成するとすると、思考時間が0だったとしても、2000秒である。しかし、これが、AIを利用して、1000文字のプロンプト（入力）を与えて3分かかったと仮定しよう。<br>
単純計算で、200+180＝380秒となって、処理時間が0.19倍となるといえばエグさが伝わるだろうか。<br>
別の言い方をするならば、30分かかっていた作業が5分もかからずに終わるようになるのである。<br>
これが積み重なると、圧倒的な差になる。これが私が感じているAIの恐怖である。<br>
AIを使う個人、企業とAIを使わない個人、企業では覆せない恐ろしいほどの差が生じるのである。

つまり、効率を考えれば考えるほど、AIを使わないと言う選択が無く、未だに「AIなんて何に使うの？」「ただのおしゃべりしかできないんでしょ？」「うちは、コードとか関係ないし」と言っているようでは、まあ、なかなか将来が厳しいかもしれないわけである。
<br>これが、私が最近感じていることであり、まあ、社会的に今後、トレンドにはなっていくだろうなとは思う。

# 技術的な解説

## そもそもtext-to-imageって何をしているの？

現在の主流である、潜在拡散モデル（Latent Diffusion Model）のアーキテクチャは、主に、以下の3ステップで構成される。

1.拡散プロセス（Diffusion Process）：ノイズから絵を復元させる「推論エンジン」

2.潜在空間（Latent Space）：計算コストを下げるための「圧縮空間」

3.条件付け（Conditioning）：テキストを画像に反映させる「制御機構」

ここから、個別にステップを解説する。

### 1.拡散プロセス

これは、熱力学の「拡散」現象に着想を得たとされる。<br>
- 順方向プロセス（Forward Process / 破壊）<br>
鮮明な画像に、徐々にガウスノイズ（正規分布に従うランダムなノイズ）を足してく。最終的には、完全な「砂嵐（ランダムノイズ）」になる。<br>
    - *イメージ:* インクの滴が水の中で拡散して、一様に混ざり合う（エントロピーが増大する）過程。<br>
- 逆方向プロセス（Reverse Process / 生成）<br>
AIが行うのはこの逆であり「今の砂嵐から、さっき足されたノイズはどれか？」を予測して引き算することを繰り返す。<br>
    - *数理:* 時刻 $t$ のノイズ画像 $x_t$ から、時刻 $t-1$ の画像 $x_{t-1}$ を事後確率 $p(x_{t-1}|x_t)$ として推定するプロセスとなる。

ここで、順方向プロセスは、学習時の方向で、逆方向は生成時の方向である。<br>
つまり、モデルが学習するのは、「**ノイズから元画像を復元する**」ことであり、「画像の描き方」を学習しているわけじゃない。

そのため、これら、生成AIの画像生成の本質を見るなら、「ノイズを除去したら、たまたま美しい絵が出てきた」とも言える。

### 2.潜在空間

高解像度（例えば：1024x1024x3色）をそのまま拡散プロセスに与えると、計算量がGPUで処理できる範囲を大幅に超えてしまう。<br>
そのため、VAE（Variational Autoencoder）という技術が使われる。<br>
これは、データを生成可能な数学的空間として、圧縮することである。<br>

こちらは、主に2ステップで構成され、エンコード（圧縮）、デコード（展開）の2ステップである。

エンコードでは、画像をピクセル空間から64x64x4程度の潜在空間に圧縮する。これは、直喩的に表現するなら、RNA-seqの生データをPCAで数個の主成分に圧縮するようなものである。

デコードでは、潜在空間でノイズを除去した後に、ピクセル空間に戻す作業を行う。

VAEが画期的な点は、通常のAutoencoderと異なり、潜在空間に確率分布として圧縮する点である。<br>
単なる固定ベクトルではなく、確率分布として圧縮することによって、連続性と正規性が保証される。<br>
VAE以前のAutoEncoder(AE)では、学習時の制約が「入出力の一致」のみであり、入力以外の箇所に関しては過学習され、離散的(不連続、非連続)な潜在空間となる。そのため入力に無い出力を行うと、データ間の点を補間できず、出力がノイズにしかならない。<br>
しかし、VAEは画像を点ではなく、確率分布（雲のようなものをイメージ）することによって、学習データ間を滑らかにつなぐ、連続的な空間が形成される。

Diffusionモデルは、単純化するとノイズからスタートして意味のある場所へ移動するプロセスである。そのため、AEを利用すると、虚無空間に入ると、勾配を見失い、生成ができない。

### 3.条件付け

ここが、テキスト（プロンプト）を画像に反映させるか？の核心である。<br>
ここには、CLIPやT5と言った、テキストエンコーダーが使われる。

これらは、以下のステップで動作する

1.テキストのベクトル化<br>
これは、入力されたテキストを多次元のベクトルに変換する

2.cross-attention<br>
ノイズ除去を行うニューラルネットワークに、画像の特徴量とテキストの特徴量を混ぜる層を設けることで、任意のテキストに従った画像の生成を可能にしている。

### 全体の生成フロー

```markdown
1.準備：テキストをベクトル化する
					↓
2.初期化：潜在空間上にランダムなノイズを用意する
					↓
3.推論ループ
・AIがノイズを見る。テキストベクトル（白い猫）を参照する (Cross-Attention)。
・「このノイズの塊、テキストに沿うなら、ここは猫の耳になるべきだから、こっちの成分はノイズだ」と予測する。
・予測されたノイズを引き算する。少し綺麗になったノイズ画像ができる。（これを繰り返す）
					↓
４．仕上げ: 完成した潜在変数を、VAEデコーダで人間が見える画像に展開する。
```

### 補足

前項でも説明しているが、実際の実用化されているstable diffusionアーキテクチャでは、以下の流れをたどる。

```markdown
【1】初期化：latent空間でランダムノイズ z₀ を生成（砂嵐）
           ↓
【2】拡散 / ノイズ除去（latent空間上で実施）
　　→ このとき cross-attention でテキストを混ぜる
           ↓
【3】latent から RGB画像へ復元
　　→ VAE Decoder で変換
           ↓
【4】出力画像
```

つまり、VAEエンコーダーは学習のみで利用され、生成時は、デコーダーしか使われない。

一応、以下に学習時の流れも同様に載せておく。

```markdown
 (学習時のみ)
画像 → VAE Encoder → latent → ノイズを足す → UNet → ノイズ除去 → VAE Decoder → 画像
```

## 代表的なtext-to-image生成AIモデル

下記に、これまでに公開されている代表的で有名なt2iモデルをまとめておく。

| 名前 | 作成者（国） | 概要（ざっくり） |
| --- | --- | --- |
| **Stable Diffusion 3** | Stability AI（英・ロンドン拠点の企業） | オープン系 t2i の代表格。拡散モデル＋Transformer（MMDiT）で、高画質かつローカル実行しやすいのが強み。SD3世代では複雑プロンプト・タイポグラフィ・構図理解がかなり改善されている。 |
| **DALL·E 3** | OpenAI（米） | ChatGPT から使える有名モデル。中身は拡散ベースの t2i だが、前段で GPT-4 がプロンプトをリライトしてから渡す構成なので、指示への忠実度が高く、ポスターやイラストもかなり狙って出せる。 |
| **Midjourney（v6/v7系）** | Midjourney, Inc.（米） | Discord から使うクローズドな t2i サービス。フォトリアル〜ファンタジー寄りの「映え絵」特化。内部仕様は非公開だが、プロンプトから直接画像を出す典型的 t2i 系列。 |
| **Imagen / Imagen 3** | Google（米） | Google の高画質 t2i モデル。Firefly や Vertex AI、Gemini API の裏側でも使われる。フォトリアルな画質と、Adobe同様 SynthID 透かしでの生成画像識別が特徴。 |
| **Ideogram 3.0** | Ideogram Inc.（カナダ） | 「画像内の文字が読める」ことをウリにした t2i。ロゴ・ポスター・看板・タイポグラフィなど文字入り画像に特化しており、多言語テキストもかなり綺麗にレンダリングできる。 |
| **Kandinsky シリーズ** | Sber / SberDevices（ロシア） | ロシア発の latent diffusion ベース t2i／i2i。多言語対応のテキストエンコーダを持ち、テキスト→画像、画像→画像、インペインティングなど一通りこなすオープン系モデル。 |
| **ERNIE-ViLG 2.0** | Baidu（中国） | 中国語プロンプトに強い大規模 t2i。24Bクラスの拡散モデルで、知識強化＋denoising experts を組み合わせた事前学習を行い、中国語圏向けのイラスト・写真・国画風などを生成できる。 |
| **通義万相（Tongyi Wanxiang）** | Alibaba Cloud（中国） | アリババの企業向け t2i/i2i。中国語と英語に対応し、水彩画・油絵・3D・アニメなど多彩なスタイル変換や、既存画像のスタイル変換をサポート。 |
| **Adobe Firefly Image 3** | Adobe（米） | Adobe の Firefly スイートに含まれる最新画像モデル。Photoshop/Illustrator/Express とガッツリ統合されており、t2i だけでなく、Generative Fill / Expand など画像編集にも最適化。Image 3世代では、構図参照・スタイル参照などコントロール性とプロンプト理解が大幅に強化され、人物・照明・複雑構造の表現がかなり良くなっている。 |
| **FLUX.1（Pro / Dev / Schnell 系）** | Black Forest Labs（ドイツ・フライブルク） | 元 Stability AI のメンバーが立ち上げた Black Forest Labs による新世代 t2i。Rectified Flow Transformer というアーキで、12B パラメータのモデル（Pro/Dev/Schnell）が展開されている。高い画質・プロンプト忠実度・タイポグラフィ性能が評価され、Cloudflare Workers AI や各種クラウド、MLPerf のベンチマークにも採用されるなど、**「現在のオープン系 t2i の新しい標準候補」**になりつつある。 |

## nano-banana proが核心的な点

nano-banana proが何がそんなに核心的だったか？について、解説したい。<br>
無論、nano-banana proが核心的なのは、タダでも使える点にもあるが、技術的にも面白いことをおこなっているようなのでそちらも解説する。

| **特徴** | **Nano Banana (従来モデル)** | **Nano Banana Pro (新モデル)** |
| --- | --- | --- |
| **正式名称** | Gemini 2.5 Flash Image | **Gemini 3 Pro Image** |
| **ベースモデル** | Gemini 2.5 Flash (軽量・高速・蒸留モデル) | **Gemini 3 Pro** (高パラメータ・推論特化モデル) |
| **生成プロセス** | Text-to-Image (直結) | **Reasoning-to-Image** (思考プロセス経由) |
| **外部知識** | モデル重み内の学習データのみ | **RAG的アプローチ** (Google検索によるGrounding) |
| **計算リソース** | 低レイテンシ・高スループット重視 | 高負荷・推論精度重視 |

こちらがかなり近いと思われる。
どうも、今回の核心的な点は、単にtext to image(t2i)を行うモデルではなく、入力プロンプトを一旦思考（thinkingもしくはReasoning）を挟んでから、生成する点であるようである。

今回、整理した感じDALL-E（openAI = chatGPTのお絵かき機能）も似た構造はあるものの、DALL-E 3は、画像生成の前段でLLMが指示を詳細化する仕組みであり、生成プロセスそのものに推論（Reasoning）が統合されているNano Banana Proとはアプローチが異なるようである。